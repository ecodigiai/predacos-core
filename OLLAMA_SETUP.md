# üß† Guia de Configura√ß√£o do Ollama - Modelos de IA Locais

Este guia explica como configurar o Ollama para executar modelos de IA localmente no Predacos, permitindo processamento de linguagem natural completamente privado e sem custos.

## üìã O que √© Ollama?

**Ollama** √© uma ferramenta que permite executar modelos de linguagem grandes (LLMs) localmente em sua m√°quina. Oferece:

- ‚úÖ **Privacidade Total** - Seus dados nunca saem da sua m√°quina
- ‚úÖ **Sem Custos** - Sem limites de API ou taxas de uso
- ‚úÖ **Offline** - Funciona sem conex√£o com a internet
- ‚úÖ **R√°pido** - Resposta em tempo real com GPU
- ‚úÖ **Customiz√°vel** - Crie modelos personalizados

## üöÄ Instala√ß√£o

### Windows

1. Baixar Ollama: https://ollama.ai/download/windows
2. Executar instalador
3. Ollama iniciar√° automaticamente
4. Verificar em: http://localhost:11434

### macOS

```bash
# Instalar via Homebrew
brew install ollama

# Ou baixar diretamente
# https://ollama.ai/download/mac
```

### Linux

```bash
# Instalar script oficial
curl https://ollama.ai/install.sh | sh

# Ou via Docker (recomendado)
docker run -d --name ollama -p 11434:11434 ollama/ollama
```

### Docker (Qualquer SO)

```bash
# Com CPU
docker run -d \
  --name ollama \
  -p 11434:11434 \
  ollama/ollama

# Com GPU NVIDIA
docker run -d \
  --name ollama \
  --gpus all \
  -p 11434:11434 \
  ollama/ollama

# Com GPU AMD
docker run -d \
  --name ollama \
  --device /dev/kfd \
  --device /dev/dri \
  -p 11434:11434 \
  ollama/ollama
```

## üì• Baixar Modelos

### Modelos Recomendados

**Para Produ√ß√£o (R√°pido e Eficiente)**

```bash
# Mistral 7B - Melhor custo-benef√≠cio
ollama pull mistral

# Neural Chat 7B - Otimizado para chat
ollama pull neural-chat

# Starling LM 7B - Bom para instru√ß√µes
ollama pull starling-lm
```

**Para M√°quinas Potentes (Melhor Qualidade)**

```bash
# Llama 2 13B - Modelo maior e mais preciso
ollama pull llama2:13b

# Llama 2 70B - Modelo gigante (requer 48GB RAM)
ollama pull llama2:70b

# Mixtral 8x7B - Modelo de mistura (requer 48GB RAM)
ollama pull mixtral
```

**Para M√°quinas Fracas (R√°pido)**

```bash
# Phi 2.7B - Muito r√°pido
ollama pull phi

# TinyLlama 1.1B - Menor modelo
ollama pull tinyllama

# Orca Mini 3B - Compacto
ollama pull orca-mini
```

### Listar Modelos Dispon√≠veis

```bash
# Ver todos os modelos
ollama list

# Exemplo de sa√≠da
# NAME              ID              SIZE    MODIFIED
# mistral:latest    2ae4d5fa4d0b    4.1 GB  2 hours ago
# neural-chat:latest 42182419e3e1   4.1 GB  1 day ago
```

### Remover Modelos

```bash
ollama rm mistral
```

## üîß Configura√ß√£o no Predacos

### 1. Vari√°veis de Ambiente

```bash
# .env
OLLAMA_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=mistral
```

### 2. Arquivo de Configura√ß√£o

```yaml
# config/independent-setup.yml
ai:
  primary:
    type: "ollama"
    model: "mistral"
    endpoint: "http://localhost:11434"
    temperature: 0.7
    max_tokens: 2048
```

### 3. Testar Conex√£o

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Exemplo de resposta
# {"models":[{"name":"mistral:latest","modified_at":"2024-01-15T10:30:00Z","size":4100000000}]}
```

## üí¨ Usar Ollama via API

### Requisi√ß√£o Simples

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Qual √© a capital da Fran√ßa?",
  "stream": false
}'
```

### Chat Interativo

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "mistral",
  "messages": [
    {
      "role": "user",
      "content": "Ol√°! Como voc√™ est√°?"
    }
  ],
  "stream": false
}'
```

### Com Streaming (Tempo Real)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Escreva um poema sobre IA",
  "stream": true
}'
```

## üìä Performance e Otimiza√ß√£o

### Requisitos de Hardware

| Modelo | RAM M√≠nimo | GPU Recomendada | Velocidade |
|--------|-----------|-----------------|-----------|
| TinyLlama 1.1B | 2GB | Nenhuma | Muito R√°pido |
| Phi 2.7B | 4GB | Nenhuma | R√°pido |
| Neural Chat 7B | 8GB | 4GB VRAM | M√©dio |
| Mistral 7B | 8GB | 4GB VRAM | M√©dio |
| Llama 2 13B | 16GB | 8GB VRAM | Lento |
| Llama 2 70B | 48GB | 24GB VRAM | Muito Lento |

### Otimizar Performance

**Usar GPU**

```bash
# NVIDIA CUDA
docker run -d \
  --name ollama \
  --gpus all \
  -p 11434:11434 \
  ollama/ollama

# AMD ROCm
docker run -d \
  --name ollama \
  --device /dev/kfd \
  --device /dev/dri \
  -p 11434:11434 \
  ollama/ollama
```

**Aumentar Contexto**

```bash
# Aumentar tamanho do contexto (mais mem√≥ria)
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "...",
  "context": 2048
}'
```

**Reduzir Temperatura** (Respostas mais determin√≠sticas)

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "...",
  "options": {
    "temperature": 0.1
  }
}'
```

## üîÑ Compara√ß√£o de Modelos

### Mistral 7B (Recomendado)

**Vantagens:**
- Excelente custo-benef√≠cio
- R√°pido e eficiente
- Bom para chat e instru√ß√µes
- 4GB VRAM

**Desvantagens:**
- Menos preciso que Llama 2 13B
- Contexto limitado

**Casos de Uso:**
- Chat interativo
- Gera√ß√£o de c√≥digo
- Resumos

### Llama 2 7B

**Vantagens:**
- Modelo open-source confi√°vel
- Bom para instru√ß√µes
- Comunidade grande

**Desvantagens:**
- Mais lento que Mistral
- Menos criativo

**Casos de Uso:**
- Tarefas estruturadas
- An√°lise de texto

### Llama 2 13B

**Vantagens:**
- Muito mais preciso
- Melhor para racioc√≠nio
- Melhor para c√≥digo

**Desvantagens:**
- Requer 16GB RAM
- Mais lento

**Casos de Uso:**
- An√°lise complexa
- Gera√ß√£o de c√≥digo avan√ßado

### Mixtral 8x7B

**Vantagens:**
- Modelo de mistura (8 especialistas)
- Muito inteligente
- Bom para m√∫ltiplas tarefas

**Desvantagens:**
- Requer 48GB RAM
- Muito lento

**Casos de Uso:**
- Tarefas muito complexas
- Pesquisa

## üéØ Dicas Pr√°ticas

### 1. Usar Modelo Certo para Tarefa

```javascript
// Para chat r√°pido
const model = "mistral";

// Para an√°lise profunda
const model = "llama2:13b";

// Para m√°quinas fracas
const model = "phi";
```

### 2. Cache de Respostas

```javascript
// Armazenar respostas em cache
const cache = new Map();

async function getResponse(prompt) {
  const key = hash(prompt);
  
  if (cache.has(key)) {
    return cache.get(key);
  }
  
  const response = await ollama.generate(prompt);
  cache.set(key, response);
  return response;
}
```

### 3. Batch Processing

```javascript
// Processar m√∫ltiplas requisi√ß√µes em paralelo
const prompts = ["...", "...", "..."];
const responses = await Promise.all(
  prompts.map(p => ollama.generate(p))
);
```

### 4. Monitorar Recursos

```bash
# Ver uso de mem√≥ria e GPU
watch -n 1 nvidia-smi

# Ver logs do Ollama
docker logs -f ollama
```

## üö® Troubleshooting

### Ollama n√£o conecta

```bash
# Verificar se est√° rodando
curl http://localhost:11434/api/tags

# Se n√£o funcionar, reiniciar
docker restart ollama

# Ou verificar logs
docker logs ollama
```

### Modelo muito lento

```bash
# Usar modelo menor
ollama pull mistral

# Ou reduzir contexto
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "...",
  "context": 512
}'
```

### Mem√≥ria insuficiente

```bash
# Usar modelo menor
ollama pull phi

# Ou aumentar swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

### GPU n√£o est√° sendo usada

```bash
# Verificar drivers
nvidia-smi

# Reinstalar Ollama com suporte a GPU
docker run -d \
  --name ollama \
  --gpus all \
  -p 11434:11434 \
  ollama/ollama
```

## üìö Recursos Adicionais

- **Site Oficial:** https://ollama.ai
- **Modelos Dispon√≠veis:** https://ollama.ai/library
- **GitHub:** https://github.com/jmorganca/ollama
- **Documenta√ß√£o API:** https://github.com/jmorganca/ollama/blob/main/docs/api.md

## üéì Pr√≥ximos Passos

1. **Instalar Ollama** - Seguir instru√ß√µes acima
2. **Baixar Modelo** - Come√ßar com `mistral`
3. **Testar API** - Fazer requisi√ß√µes curl
4. **Integrar ao Predacos** - Configurar vari√°veis de ambiente
5. **Otimizar** - Ajustar temperatura e contexto
6. **Monitorar** - Acompanhar performance

---

**Desenvolvido para m√°xima privacidade e independ√™ncia!**
